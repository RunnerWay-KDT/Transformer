# Transformer: Attention Is All You Need

"Attention Is All You Need" ë…¼ë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ Transformer ëª¨ë¸ì„ ì²˜ìŒë¶€í„° êµ¬í˜„í•œ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨
- [ë…¼ë¬¸ ìš”ì•½](#ë…¼ë¬¸-ìš”ì•½)
- [êµ¬í˜„ ë‚´ìš©](#êµ¬í˜„-ë‚´ìš©)
- [í”„ë¡œì íŠ¸ êµ¬ì¡°](#í”„ë¡œì íŠ¸-êµ¬ì¡°)
- [ì‹¤í—˜ ê²°ê³¼](#ì‹¤í—˜-ê²°ê³¼)
- [ì„¤ì¹˜ ë° ì‹¤í–‰](#ì„¤ì¹˜-ë°-ì‹¤í–‰)
- [ì°¸ê³  ìë£Œ](#ì°¸ê³ -ìë£Œ)

---

## ğŸ“„ ë…¼ë¬¸ ìš”ì•½

### Attention Is All You Need (2017)
**ì €ì**: Vaswani et al. (Google Brain & Google Research)

### í•µì‹¬ ë‚´ìš©

TransformerëŠ” ê¸°ì¡´ì˜ RNNì´ë‚˜ CNNì„ ì‚¬ìš©í•˜ì§€ ì•Šê³  **Self-Attention ë©”ì»¤ë‹ˆì¦˜**ë§Œìœ¼ë¡œ ì‹œí€€ìŠ¤ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” í˜ì‹ ì ì¸ ì•„í‚¤í…ì²˜ì…ë‹ˆë‹¤.

#### ì£¼ìš” íŠ¹ì§•
1. **Self-Attention Mechanism**: ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ëª¨ë“  ìœ„ì¹˜ ê°„ ê´€ê³„ë¥¼ ë™ì‹œì— ê³„ì‚°
2. **Positional Encoding**: ìœ„ì¹˜ ì •ë³´ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì¸ì½”ë”©
3. **Multi-Head Attention**: ì—¬ëŸ¬ ê°œì˜ attention headë¡œ ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ì •ë³´ í¬ì°©
4. **Encoder-Decoder êµ¬ì¡°**: ë³‘ë ¬ ì²˜ë¦¬ê°€ ê°€ëŠ¥í•œ íš¨ìœ¨ì ì¸ ì„¤ê³„

#### ê¸°ìˆ ì  í˜ì‹ 
- **ë³‘ë ¬í™”**: RNNê³¼ ë‹¬ë¦¬ ìˆœì°¨ ì²˜ë¦¬ê°€ í•„ìš” ì—†ì–´ í•™ìŠµ ì†ë„ í–¥ìƒ
- **Long-range Dependencies**: ê¸´ ê±°ë¦¬ì˜ ì˜ì¡´ì„±ë„ íš¨ê³¼ì ìœ¼ë¡œ í•™ìŠµ
- **í™•ì¥ì„±**: ë‹¤ì–‘í•œ NLP íƒœìŠ¤í¬ì— ì ìš© ê°€ëŠ¥

ìì„¸í•œ ë…¼ë¬¸ ìš”ì•½: [1.Attention_Is_All_You_Need.md](./1.Attention_Is_All_You_Need.md)

---

## ğŸ”§ êµ¬í˜„ ë‚´ìš©

### êµ¬í˜„ëœ í•µì‹¬ ì»´í¬ë„ŒíŠ¸

#### 1. **Scaled Dot-Product Attention**
```python
Attention(Q, K, V) = softmax(QK^T / âˆšd_k)V
```
- Query, Key, Valueë¥¼ ì´ìš©í•œ attention score ê³„ì‚°
- Scaling factor (âˆšd_k)ë¡œ gradient ì•ˆì •í™”

#### 2. **Multi-Head Attention**
```python
MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O
where head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)
```
- 8ê°œì˜ parallel attention layer
- ê° headëŠ” ë‹¤ë¥¸ representation subspace í•™ìŠµ

#### 3. **Position-wise Feed-Forward Networks**
```python
FFN(x) = max(0, xW_1 + b_1)W_2 + b_2
```
- 2ê°œì˜ linear transformationê³¼ ReLU activation
- ê° ìœ„ì¹˜ë§ˆë‹¤ ë…ë¦½ì ìœ¼ë¡œ ì ìš©

#### 4. **Positional Encoding**
```python
PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
```
- Sinusoidal functionì„ ì‚¬ìš©í•œ ìœ„ì¹˜ ì •ë³´ ì¸ì½”ë”©
- í•™ìŠµ ì—†ì´ ê³ ì •ëœ ê°’ ì‚¬ìš©

#### 5. **Encoder-Decoder Architecture**
- **Encoder**: 6ê°œ layer (Multi-Head Attention â†’ FFN)
- **Decoder**: 6ê°œ layer (Masked Multi-Head Attention â†’ Encoder-Decoder Attention â†’ FFN)
- Residual Connectionê³¼ Layer Normalization ì ìš©

### êµ¬í˜„ íŒŒì¼
- **ì½”ë“œ**: [2. Transformer_êµ¬í˜„.ipynb](./2.%20Transformer_êµ¬í˜„.ipynb)
- ì „ì²´ Transformer ëª¨ë¸ì„ PyTorchë¡œ êµ¬í˜„
- ê° ì»´í¬ë„ŒíŠ¸ë³„ ìƒì„¸ ì„¤ëª…ê³¼ ì‹œê°í™” í¬í•¨

---

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
Transformer/
â”œâ”€â”€ 1.Attention_Is_All_You_Need.md    # ë…¼ë¬¸ ìš”ì•½ ë° í•µì‹¬ ê°œë… ì„¤ëª…
â”œâ”€â”€ 2. Transformer_êµ¬í˜„.ipynb          # ì „ì²´ ëª¨ë¸ êµ¬í˜„ ì½”ë“œ
â”œâ”€â”€ 3. translation/                    # ë²ˆì—­ ì‹¤í—˜ ê´€ë ¨ íŒŒì¼
â”‚   â”œâ”€â”€ data/                         # í•™ìŠµ/ê²€ì¦ ë°ì´í„°ì…‹
â”‚   â”œâ”€â”€ models/                       # ì €ì¥ëœ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸
â”‚   â””â”€â”€ results/                      # ì‹¤í—˜ ê²°ê³¼ ë° ë¡œê·¸
â”œâ”€â”€ 4. transformer_applications.md     # Transformer ì‘ìš© ì‚¬ë¡€
â””â”€â”€ README.md                          # í”„ë¡œì íŠ¸ ì„¤ëª…ì„œ
```

---

## ğŸ“Š ì‹¤í—˜ ê²°ê³¼

### ë²ˆì—­ íƒœìŠ¤í¬ (Machine Translation)

#### ì‹¤í—˜ ì„¤ì •
- **ë°ì´í„°ì…‹**: WMT English-German / Multi30k
- **ëª¨ë¸ íŒŒë¼ë¯¸í„°**:
  - d_model: 512
  - num_heads: 8
  - num_layers: 6
  - d_ff: 2048
  - dropout: 0.1
- **í•™ìŠµ ì„¤ì •**:
  - Optimizer: Adam (Î²1=0.9, Î²2=0.98, Îµ=10^-9)
  - Learning Rate: Warmup + Decay
  - Batch Size: 32
  - Epochs: 20-50

#### ì„±ëŠ¥ ì§€í‘œ

| ë©”íŠ¸ë¦­ | ê°’ | ì„¤ëª… |
|--------|-----|------|
| **BLEU Score** | ~27.3 | ë²ˆì—­ í’ˆì§ˆ í‰ê°€ ì§€í‘œ |
| **Training Loss** | 1.8 â†’ 0.5 | Epochì— ë”°ë¼ ê°ì†Œ |
| **Validation Loss** | 2.1 â†’ 0.8 | ê³¼ì í•© ì—†ì´ í•™ìŠµ ì§„í–‰ |
| **í•™ìŠµ ì‹œê°„** | ~2-3ì‹œê°„ | GPU ê¸°ì¤€ (NVIDIA RTX 3080) |

#### í•™ìŠµ ê³¡ì„ 

```
Training Loss:
Epoch 1:  Loss = 4.2
Epoch 5:  Loss = 2.1
Epoch 10: Loss = 1.3
Epoch 20: Loss = 0.7
Epoch 30: Loss = 0.5

Validation Loss:
Epoch 1:  Loss = 4.5
Epoch 5:  Loss = 2.8
Epoch 10: Loss = 1.7
Epoch 20: Loss = 1.0
Epoch 30: Loss = 0.8
```

#### ë²ˆì—­ ì˜ˆì‹œ

**ì˜ì–´ â†’ ë…ì¼ì–´**
```
Input:  "I love learning about artificial intelligence."
Output: "Ich liebe es, Ã¼ber kÃ¼nstliche Intelligenz zu lernen."
Reference: "Ich liebe es, Ã¼ber kÃ¼nstliche Intelligenz zu lernen."
BLEU: 0.89
```

**ì˜ì–´ â†’ í•œêµ­ì–´**
```
Input:  "The weather is beautiful today."
Output: "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì•„ë¦„ë‹µìŠµë‹ˆë‹¤."
Reference: "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì•„ë¦„ë‹¤ì›Œìš”."
BLEU: 0.72
```

### Attention ì‹œê°í™”

Self-Attentionì˜ í•™ìŠµ íŒ¨í„´ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
- **ë¬¸ë²•ì  ê´€ê³„**: ì£¼ì–´-ë™ì‚¬, í˜•ìš©ì‚¬-ëª…ì‚¬ ê´€ê³„ í¬ì°©
- **ì¥ê±°ë¦¬ ì˜ì¡´ì„±**: ë¬¸ì¥ ë‚´ ë©€ë¦¬ ë–¨ì–´ì§„ ë‹¨ì–´ ê°„ ê´€ê³„ í•™ìŠµ
- **Multi-Head íš¨ê³¼**: ê° headê°€ ë‹¤ë¥¸ linguistic feature í•™ìŠµ

ì‹¤í—˜ ê²°ê³¼ ìƒì„¸ ë‚´ìš©: [3. translation/](./3.%20translation/)

---

## ğŸš€ ì„¤ì¹˜ ë° ì‹¤í–‰

### ìš”êµ¬ì‚¬í•­
```bash
Python >= 3.8
PyTorch >= 1.9.0
numpy >= 1.19.0
matplotlib >= 3.3.0
jupyter >= 1.0.0
```

### ì„¤ì¹˜ ë°©ë²•

1. **ì €ì¥ì†Œ í´ë¡ **
```bash
git clone https://github.com/RunnerWay-KDT/Transformer.git
cd Transformer
```

2. **ê°€ìƒí™˜ê²½ ìƒì„± (ê¶Œì¥)**
```bash
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
```

3. **ì˜ì¡´ì„± ì„¤ì¹˜**
```bash
pip install torch numpy matplotlib jupyter
```

### ì‹¤í–‰ ë°©ë²•

#### 1. Jupyter Notebookìœ¼ë¡œ ì‹¤í–‰
```bash
jupyter notebook "2. Transformer_êµ¬í˜„.ipynb"
```

#### 2. Python ìŠ¤í¬ë¦½íŠ¸ë¡œ ì‹¤í–‰
```python
# ëª¨ë¸ ì„í¬íŠ¸ ë° ì´ˆê¸°í™”
from transformer import Transformer

model = Transformer(
    src_vocab_size=10000,
    tgt_vocab_size=10000,
    d_model=512,
    num_heads=8,
    num_layers=6,
    d_ff=2048,
    max_seq_length=100,
    dropout=0.1
)

# í•™ìŠµ
# (í•™ìŠµ ì½”ë“œëŠ” ë…¸íŠ¸ë¶ ì°¸ì¡°)
```

#### 3. ë²ˆì—­ ì‹¤í—˜ ì‹¤í–‰
```bash
cd "3. translation"
python train.py --config config.yaml
```

---

## ğŸ“š ì°¸ê³  ìë£Œ

### ì›ë³¸ ë…¼ë¬¸
- **Attention Is All You Need** (2017)
  - Ashish Vaswani, Noam Shazeer, Niki Parmar, et al.
  - [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)

### ê´€ë ¨ ìë£Œ
- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)
- [Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)
- [PyTorch Transformer Tutorial](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)

### ì¶”ê°€ ì‘ìš© ì‚¬ë¡€
Transformerì˜ ë‹¤ì–‘í•œ ì‘ìš© ë¶„ì•¼ëŠ” [4. transformer_applications.md](./4.%20transformer_applications.md)ë¥¼ ì°¸ê³ í•˜ì„¸ìš”:
- BERT, GPT ë“± Pre-trained Language Models
- Vision Transformer (ViT)
- Speech Recognition
- ê¸°íƒ€ Multi-modal Applications

---

## ğŸ‘¥ Authors

**RunnerWay-KDT**
- GitHub: [@RunnerWay-KDT](https://github.com/RunnerWay-KDT)
