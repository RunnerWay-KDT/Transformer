## 🏷️ 논문 개요

### "Attention Is All You Need" 의 의미

시퀀스 모델링 및 변환 작업에서 기존의 표준이었던 **순환 신경망(RNN)이나 합성곱(CNN)과 같은 복잡한 구조를 더 이상 사용할 필요가 없으며, 오직 '어텐션(Attntion)' 매커니즘만으로도 충분하다**는 파격적인 선언을 담고있다.

과거에는 어텐션이 주로 순환 네트워크와 결합하여 보조적으로 사용되었으나, 이 논문은 어텐션만을 활용해 전역적 의존성을 모델링함으로써 **병렬 처리를 극대화** 하고 **학습 시간을 획기적으로 단축** 할 수 있음을 입증했다.

### 한 줄 요약

> 순환 신경망(RNN)이나 합성곱 신경망(CNN) 없이 오직 Self-Attention 메커니즘만을 사용하는 새로운 아키텍처인 Transformer를 제안하여, 기계 번역 작업에서 최고 성능을 달성하면서도 병렬화가 가능하고 학습 시간을 대폭 단축했다.

## Abstact

지배적인 시퀀스 변환 모델들은 인코더와 디코더를 포함하는 복잡한 순환 신경망(recurrent) 또는 합성곱 신경망(convolutional)을 기반으로 한다. 가장 뛰어난 성능을 보이는 모델들 역시 어텐션 매커니즘(attention mechanism)을 통해 인코더와 디코더를 연결한다. 우리는 순한(recurrence)과 합성곱(convolutions)을 완전히 배재하고, 오로지 어텐션 매커니즘에만 기반한 새로운 단순한 네트워크 아키텍처인 '트랜스포머(Transforemer)'를 제안한다. 두 가지 기계 번역 작업에 대한 실험은 이 모델들이 품질 면에서 우수하면서도, 병렬화가 훨씬 용이하고 훈련에 필요한 시간을 현저히 적게 소모한다는 것을 보여준다. 우리 모델은 WMT 2014 영어-독일어 번역 작업(WMT 2014 Englishto-German translation task)에서 28.4 BLEU(Bilingual Evaluation Understudy, 기계 번역 모델이 만든 결과물이 사람이 번역한 정답과 얼마나 유사한지를 측정하는 자연어 처리(NLP) 분야의 대표적인 평가 지표)를 달성하며, 앙상블 모델을 포함한 기존의 최고 기록들은 2 BLEU 이상 경신했다. WMT 2014 영어-프랑스어 번역 작업(WMT 2014 English-to-French translation task) 에서는 8개의 GPU로 3.5일 동안 훈련한 후, 문헌상의 최고 모델들이 사용한 훈련 비용의 아주 작은 부분만으로도 41.8이라는 새로운 단일 모델 최고(state-of-the-art) BLEU 점수를 기록했다. 우리는 대규모 훈련 데이터와 제한된 훈련 데이터 모두를 사용한 영어 구문 분석(constituency parsing)에 성공적으로 적용함으로써, 트랜스포머가 다른 작업들에도 일반화가 잘 된다는 것을 보여준다.

<br />

---

## 🔍 연구 배경: 시퀀스 모델링의 역사적 맥락

### RNN/LSTM의 등장과 한계

- 자연어는 본질적으로 순차적(sequential) 데이터이다.
- RNN과 LSTM/GRU는 이전 시점의 정보를 'hidden state'로 전달하며 시퀀스를 처리했다.
- 하지만 **순차적 처리의 본질적 한계** 로 인해:
    - 병렬 처리가 불가능하여 학습 속도가 느리고,
    - 긴 시퀀스에서 경사 소실 및 경사 폭발 (gradient vanishing/exploding) 문제 발생
    - 먼 거리의 단어 간 의존성을 효과적으로 학습하기 어려움

### Attention 매커니즘의 등장

[attention mechanism 자세히](../AI/2026-01-16-attension-mechanism.md)

- Banhadanau et al.(2015)이 제안한 **Attention** 은 RNN의 **고정 길이 컨텍스트 벡터 문제(Fixed-length Context Vector)** 를 해결
- 디코더가 인코더의 모든 시점을 참조할 수 있게 하여 성능 향상
- 하지만 여전히 RNN을 기반으로 했기 때문에 **병렬화 제약** 은 남아있었다.

### CNN기반 접근의 시도

- ConvS2S, ByteNet 등은 CNN을 사용하여 병렬 처리 가능
- 하지만 장거리 의존성(Long-Term Dependency)을 학습하기 위해 여러 층을 쌓아야 했고, 이는 계산 비용 증가로 이어졌다.
-

### 2017년 당시의 기술적 상황과 트랜스포머의 등장

- **GPU 기술의 발전**: NVIDIA P100과 같은 강력한 GPU를 8개씩 활용하여 대규모 병렬 연산이 가능한 환경이 구축되었다.
- **대규모 데이터의 확보**: WMT 2014 영-독(450만 문장 쌍) 및 영-불(3,600만 문장 쌍)과 같은 대규모 코퍼런스를 학습에 활용할 수 있게 되었다. (대규모 병렬 코퍼스 확보)
- **연구 동향(새로운 아키텍처의 수요)**: 병렬 처리가 가능하면서도 모든 위치 간의 관계를 **상수 번($O(1)$)의 연산** 만으로 직접 연결할 수 있는 새로운 구조에 대한 필요성이 대두되었다.

<br />

---

## 🔑 핵심 키워드

### 셀프 어텐션 (Self-Attention)

하나의 시퀀스 내에서 **서로 다른 위치들 간의 관계를 계산** 하여 시퀀스의 **표현(Representation)** 을 만들어내는 매커니즘이다.

#### 직관적 이해

각 토큰은 다른 모든 토큰을 '얼마나 볼지'를 가중치로 계산해, 중요한 단어는 크게, 덜 중요한 단어는 작게 반영해 새로운 표현을 만든다. "사과" 처럼 문맥에 따라 달라지는 의미를 스스로 연결해 파악할 수 있다.

예를 들어 "The animal didn't cross the stree because **it** was too tired" 에서 "it" 이 "animal" 을 가리킨다는 관계를 자동으로 학습할 수 있다.

#### 작동 매커니즘

Self-Attention은 입력 임베딩을 이용해 단어들 사이의 관계를 수치적으로 계산한다.

- **유사도 계산**: 각 단어의 임베딩 벡터로 행렬을 만들고, 하나는 전치(transpose) 하여 곱셈을 수행한다. 이를 통해 각 단어가 다른 단어와 얼마나 관련이 있는지를 나타내는 유사도 행렬이 만들어진다.
- **가중합 계산**: 이전 단계에서 얻은 유사도 행렬에 Value 행렬을 곱해준다. 예를 들어 "커피 한잔 어때?" 라는 문장에서 '커피'라는 단어 벡터는 "한잔"이나 "어때" 같은 단어와의 가중치를 곱해 더해진 최종 벡터로 변환 된다.

#### 수식

$$Attention(Q, K, V) = softmax(QK^T / √d_k)V$$

#### 핵심 특징

- 모든 단어 쌍의 관계를 **한 번에 행렬 연산**으로 계산
- 순차 처리가 아니므로 **완전한 병렬화** 가능
- 거리에 관계없이 모든 단어를 직접 참조 가능

### Query, Key, Value(쿼리, 키, 값)

Self-Attention은 입력 임베딩에서 **Query(쿼리), Key(키), Value(밸류)** 라는 세 가지 행렬을 만들어 낸다.

- **Query**: 질문을 던지는 역할 (ex. "나는 어떤 단어에 주목해야 하나?")
- **Key**: 단서를 제공하는 역할 (ex. "나는 이런 정보를 가지고 있어.")
- **Value**: 실제 답변 역할 (ex. "이것이 전달할 실제 내용이야.")

### 멀티 헤드 어텐션 (Multi-Head Attention)

Self-Attention을 병렬로 여러 번 수행하는 구조이다. 모델이 서로 다른 위치에 있는 **다양한 표현 하위 공간(representation subspaces)으로부터 공동으로 정보를 참조** 할 수 있게 하며, 단일 어텐션에서 정보가 평균화되어 세밀한 특징이 억제되는 것을 방지한다. 마치 여러 명의 전문가가 각자 다른 관점에서 문장을 분석하는 것과 유사하다.

#### 왜 필요한가?

- 단일 Attention은 어텐션 가중치가 적용된 위치들의 값을 평균 내어 출력하므로, 서로 다른 성격의 정보들을 하나의 가중 합으로 뭉뚱그려 처리하게 된다.
- 이 과정에서 특정 정보가 다른 정보에 의해 가리워지거나 억제(inhibits)되는 문제가 발생한다.
- Multi-Head는 **다양한 표현 하위 공간(representation subspaces)으로부터 공동으로 정보를 참조** 할 수 있게 설계되었다.
- 예: 한 헤드는 문법 구조, 다른 헤드는 대명사의 지칭 대상, 또 다른 헤드는 의미적 관계를 학습하는 식이다.

### 병렬화 (Parallelization)

트랜스포머의 가장 큰 기술적 장점 중 하나이다. RNN의 본질적인 순차적 특성 때문에 불가능했던 **훈련 과정에서의 병렬 처리** 를 가능하게 하여, 훨씬 적은 시간으로 대규모 모델을 학습시킬 수 있게 한다.

### Residual Connection & Layer Normalization

#### Residual Connection (잔차 연결)

$$output = LayerNorm(x + Sublayer(x))$$

- 깊은 네크워크에서 gradient flow를 원할하게 함
- 각 층이 입력을 완전히 변환하는 것이 아니라 변화량 (residual)만 학습

#### Layer Normalization

- 각 층의 출력을 정규화하여 학습 안정화
- Batch Normalization과 달리 시퀀스 길이에 독립적

### 스케일드 닷-프로덕트 어텐션 (Scaled Dot-Product Attention)

트랜스포머에서 사용하는 구체적인 어텐션 계산 방식이다. 쿼리(query)와 키(key)의 내적(dot product)을 계산하되, **차원($d_k$)의 제곱근으로 나누어 스케일링** 함으로써 값이 너무 커져서 발생하는 **기울기 소실 문제를 방지** 한다.

### 장거리 의존성 (Long-range Dependencies)

시퀀스 데이터 내에서 멀리 떨어진 요소들 간의 관계를 학습하는 능력이다. 트랜스포머는 **모든 위치 사이의 경로 길이를 상수로 고정함**으로써, RNN이나 CNN 기반 모델보다 **장거리 의존성을 훨씬 효율적으로 학습** 한다.

### 시퀀스 변환 (Sequence Transduction)

기계 번역이나 구문 분석과 같이 하나의 시퀀스를 다른 시퀀스로 변환하는 작업의 통칭이다. 트랜스포머는 이 분야에서 **최첨단(State-of-the-art) 성능**을 달성했다.

<br />

---

## 🚫 기존 방법의 한계

### 기존 순환 모델(RNN, LSTM, GRU)의 한계

- **순차적 계산의 제약**: 순환 모델은 입력 및 출력 시퀀스의 기호 위치에 따라 계산을 분할하며, 이전 은닉 상태 $h_{t-1}$ 과 현재 위치 $t$ 의 입력을 결합하여 현재 은닉 상태 $h_t$ 를 생성하는 **본질적으로 순차적인 특성** 을 가진다.
- **병렬화 불가능**: 이러한 순차적 성격은 훈련 단계에서 **예제 내의 병렬화를 불가능**하게 만든다.
- **메모리 제약 및 효율성 저하**: 시퀀스 길이가 길어질수록 메모리 제약으로 인해 예제 간 배치 처리가 제한되며, 이는 계산 효율성을 심각하게 떨어뜨린다.
- **장거리 의존성 학습의 어려움**: 두 위치 사이를 연결하기 위해 거쳐야 하는 순차적 연산 수가 시퀀스 길이 $n$에 비례하여 ($O(n)$) 늘어나기 때문에, 멀리 떨어진 위치 간의 의존성을 학습하는 것이 매우 어렵다.

### 합성곱 모델(CNN 기반 모델)의 시도와 한계

- **거리 비례 연산 증가**: ConvS2S나 ByteNet과 같은 모델은 은닉 표현을 병렬로 계산하지만, 임의의 두 위치 사이의 신호를 연결하는 데 필요한 **연산 수가 위치 간 거리에 따라 선형적 또는 로그적으로 증가** 한다.
- **장거리 의존성 포착의 비효율성**: 거리에 따라 연산량이 늘어나는 구조는 시퀀스 내에서 **멀리 떨어진 요소들 간의 관계를 학습하는 것을 더 어렵게** 만든다.
- **복잡한 층 구조 요구**: 모든 입력 및 출력 위치 쌍을 연결하기 위해서는 여러 층의 합성곱을 쌓아야 하므로, 네트워크의 최대 경로 길이가 길어지는 결과를 초래한다.

### 기존 어텐션 활용 방식의 한계

- **보조적 역할의 국한**: 트랜스포머 이전에도 어텐션 매커니즘은 시퀀스 모델링의 중요한 부분이었으나, 대부분의 경우 **순환 네트워크와 결합된 형태** 로만 사용되었다.
- **정보 손실의 위험**: 단일 어텐션 헤드만 사용할 경우, 여러 위치의 정보를 평균화하는 과정에서 다양한 하위 공간의 정보를 세밀하게 포착하지 못하고 억제될 위험이 있습니다

| 특징          | 기존 모델 (RNN / CNN)                         | 트랜스포머 (Transformer)              |
| ------------- | --------------------------------------------- | ------------------------------------- |
| 계산 방식     | 순차적(Sequential) 처리                       | 병렬적(Parallel) 처리                 |
| 병렬화 가능성 | 낮음 (이전 시점 계산이 끝나야 다음 계산 가능) | 매우 높음 (모든 단어를 동시에 계산)   |
| 장거리 의존성 | 거리가 멀수록 정보 소실 발생 (O(n))           | 거리와 상관없이 즉각 연결 가능 (O(1)) |
| 학습 효율성   | 계산 비용 대비 성능 향상이 느림               | 학습 시간 획기적 단축 및 고성능 달성  |

<br />

---

## 🧭 핵심 아이디어: 왜 트랜스포머인가?

트랜스포머 모델 아키텍처 (Transformer Model Architecture)

### Encoder and Decoder Stacks

<img width="100%" alt="Image" src="https://github.com/user-attachments/assets/b408f8d1-5ef9-4510-b3fe-f17293af9b62" />

#### 인코더 (Encoder)

- **6층의 스택**: 동일한 층 N = 6개를 쌓아 정보를 정교하게 추출한다.
- **잔차 연결과 정규화**: 각 서브 레이어의 출력에 $LayerNorm(x+Sublayer(x))$를 적용하여 정보 손실을 막고 안정적 학습을 돕는다.
- **512차원의 통일 ($d_{model}$)**: 모든 서브 레이어와 임베딩의 출력 차원(모든 층의 입출력 크기)을 $d_{model} = 512$로 고정하여 구조적 일관성을 유지한다.

#### 디코더 (Decoder)

- **인코더-디코더 어텐션**: 디코더의 세 번째 서브 레이어로, 인코더의 출력값(K, V)을 사용하여 입력 문장의 적절한 위치를 참조한다.
- **마스킹(Masking)**: 현재 위치 이후의 정보를 보지 못하도록 가려(Masked Multi-Head Attention), 모델이 '미래 정보'를 이용해 치팅하는 것을 방지한다.
- **자기회귀(Auto-Regressive)**: 예측된 출력을 다시 입력으로 소모하며 한 번에 하나의 심볼을 생성하는 방식을 수학적으로 보존한다.

### Attention

<img width="100%" alt="Image" src="https://github.com/user-attachments/assets/4b9600e6-040e-4a9b-9ac9-0d7184ed62d8" />

#### Scaled Dot-Product Attention

- **내적 기반 호환성**: $Q$(쿼리) 와 $K$(키)의 내적을 통해 유사도를 계산하고, 이를 가중치로 삼아 $V$(값)의 합을 구한다.
- **스케일링($\frac{1}{\sqrt{d_k}}$)**: 차원 $d_k$ 가 커질 때 내적값이 비대해져 Softmax의 기울기가 소실되는 것을 방지하기 위해 $1/\sqrt{d_k}$ 로 나누어 보정한다.
- **효율적 연산**: Additive Attention에 비해 행렬 곱셈 연산을 활용할 수 있어 속도가 빠르고 공간 효율적이다.
- **수식**:

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

#### Multi-Head Attention

- **병렬적 투영**: 단일 어텐션을 수행하는 대신, $Q, K, V$ 를 서로 다르게 학습된 $h$ 개의 선형 투영(Linear Projection, 행렬 곱)을 통해 $h$ 개의 개별 어텐션(Head)을 병렬로 수행한다.
- **다각적 정보 수집**: 여러 개의 헤드를 통해 서로 다른 표현 하위 공간(Subspaces)으로부터 문맥 정보를 동시에 포착하여 정보의 손실을 막는다.
- **차원 분할과 통합**: 각 헤드는 $d_k = d_{model}/h$차원을 가지며, 결과값들을 연결(Concatenate)한 후 최종 투영 행렬 $W^O$를 곱해 출력한다.
- **설정**: 본 연구에서는 $h=8$개의 헤드를 사용하며, 각 헤드 차원을 줄여 전체 계산 비용을 단일 헤드 어텐션과 비슷하게 유지한다.
- **수식**:

$$
\begin{align*}
\operatorname{MultiHead}(Q, K, V) &= \operatorname{Concat}(\text{head}_1, \dots, \text{head}_h) W^O \\
\text{where } \text{head}_i &= \operatorname{Attention}(QW_i^Q, KW_i^K, VW_i^V)
\end{align*}
$$

### Feed-Forward Network (FFN)

각 위치에 독립적으로 적용되는 2층 완전 연결 신경망이다. Attention으로 모은 정보를 비선형 변환하여 더 풍부한 표현을 학습한다.

$$FFN(x) = max(0, xW₁ + b₁)W₂ + b₂$$

#### 역할

- Attention이 정보를 모으는 역할이라면, FFN은 정보를 변환하는 역할
- 각 위치마다 독립적으로 적용되어 병렬화 가능
- ReLU 활성화로 비선형성 제공

### Embeddings and Softmax

### 포지셔널 인코딩 (Positional Encoding)

<img width="100%" src='https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdna%2Fc08NZL%2FbtsQaSqMio7%2FAAAAAAAAAAAAAAAAAAAAAFVOuAPuIYgYdcpBRhlKVYB6IkD5PL7YgeLxLRDHU1mF%2Fimg.png%3Fcredential%3DyqXZFxpELC7KVnFOS48ylbz2pIh7yKj8%26expires%3D1769871599%26allow_ip%3D%26allow_referer%3D%26signature%3DNzCX7hyWToRaNW3Ha07eubpHXEg%253D' />

Transformer는 순환 구조가 없기 때문에 발생하는 **순서 정보의 부재** 를 해결하기 위한 기술이다. Positional Encoding은 각 토큰의 위치 정보를 사인(sine)과 코사인(cosine) 함수를 사용하여 토큰의 상대적 또는 절대적 위치 정보를 입력 임베딩에 주입한다.

#### 수식

$$PE(pos, 2i) = sin(pos / 10000^(2i/d_model))$$
$$PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))$$

#### 핵심 특징

- 상대적 위치를 선형 변환으로 표현 가능
- 학습 데이터보다 긴 시퀀스에도 일반화 가능
- 주기성을 가져 패턴 학습 용이

<br />

---

### 셀프 어텐션을 선택한 이유 (Why Self-Attention)

<img width="100%" alt="Image" src="https://github.com/user-attachments/assets/49242fdf-e8bd-4126-b73f-1f42d6cae4be" />

#### 레이어당 총 계산 복잡도 (Computational Complexity)

- **효율성**: 시퀀스 길이 $n$이 모델의 차원 $d$보다 작을 때 ($n < d$), 셀프 어텐션은 RNN보다 연산량이 적다.
- **현실적 이점**: 최신 번역 모델에서 사용하는 단어 단위(Word-piece) 나 서브워드(Byte-pair) 표현에서는 대부분 문장 길이($n$)가 차원($d = 512$)보다 짧기 때문에 연산 효율이 극대화된다.

#### 병렬화 가능한 계산 (Parallelizability)

- **상수 시간 연산**: RNN은 이전 단어의 계산이 끝나야 다음 단어를 계산할 수 있는 순차적($O(n)$) 구조인 반면, 셀프 어텐션은 모든 단어 간의 관계를 **한 번에($O(1)$)** 계산할 수 있어 GPU 병렬 처리에 최적화되어 있다.
- **속도**: 하드웨어 가속기를 최대한 활용할 수 있어 학습 속도가 획기적으로 빠르다.

#### 장거리 의존성 학습 (Long-Range Dependencies)

- **최단 경로**: 두 지점 사이의 경로가 짧을수록 장거리 의존성을 학습하기 쉽다. 셀프 어텐션은 임의의 위치 쌍을 **단 한 번의 연결($O(1)$)**로 잇는다.
- **제한된 셀프 어텐션 (Restricted Self-Attention)**: 매우 긴 시퀀스에서는 각 위치를 중심으로 크기 $r$의 이웃 영역만 고려하도록 제한할 수 있으며, 이 경우 최대 경로 길이는 $O(n/r)$이 된다.
- **Convolutional 레이어와의 비교**:
    - **커널 폭이 $k < n$인 단일 CNN** 은 모든 위치를 연결하기 위해 다량의 층을 쌓거나($O(n/k)$),
    - **다이레이션(Dilated) 기법($O(\log_k n)$)**을 사용해야 하므로 경로 길이가 늘어난다.
    - 하지만 **Separable Convolution**을 사용할 경우 복잡도를 대폭 낮출 수 있으며, $k=n$일 경우에도 self-attention 레이어와 point-wise feed-forward 레이어의 조합과 유사한 복잡도를 가진다.

#### 부수적 이점: 해석 가능성 (Interpretability)

- **개별 헤드의 전문화(Specialization)**: 멀티 헤드 어텐션의 각 헤드는 단순히 무작위로 학습되는 것이 아니라, 서로 다른 특정 작업을 수행하도록 명확하게 분업화된다.
- **구조적 학습**: 개별 어텐션 헤드는 문장의 구문론적(Syntactic), 의미론적(Semantic) 구조와 관련된 동작을 보였다.

<br />

---

## 📜 참고 자료

- 원논문: [AttentionIsAllYouNeed](https://arxiv.org/pdf/1706.03762)
